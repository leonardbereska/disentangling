\subsection{framework}\label{sec:framework} \begin{comment} % crossing task framework \begin{figure}[t] \centering \includegraphics[trim={0cm 0cm 0cm 0cm},clip, width=1.\linewidth]{fig/frameworkz} \caption{encoder $e$ encodes shape and appearance for two images $\pi(x)$ and $\phi(x)$, after recombination $r$ of $(mathbf{f}{\pi(x)}, mathbf{p}{\phi(x)})$ into latent image $z$, the decoder $d$ reconstructs the image $x$.} \label{fig:overview} \end{figure} \end{comment} %definitions we want to represent an object in an image $x$ in $n$ parts. let us denote the part shape of part $i$ with $p^ix$ and the part appearance with $f^ix$. the total object shape is constituted by the collection of its part shapes $\mathbf{p}x = (p^1x, ..., p^nx)$, the same goes for the total appearance $\mathbf{f}x = (f^1x, ..., f^nx)$. we model the part appearances as feature vectors, the part shapes are chosen to be scalar fields like the image itself. thereby one can establish a direct correspondence of locations in the image to locations in the shape representation.\todo{say the following or not?}\ %in contrast to other works, where object pose is often modeled by single points as landmarks, a representation based on a scalar field is much richer: whereas a landmarks inform about the exact position, the field-like representation also describes the size and shape of the object parts (see fig.\ref{fig:overview}). % transformations -> crossing framework how do we disentangle the shape and appearance components in the representation? in general, a variation in shape will not affect appearance and vice versa. thus, if we deliberately change shape without changing appearance, we can enforce the invariance of the appearance representation under such a change. we refer to these changes as shape transformations $\pi$, which, if applied to an image $x$, directly act on the underlying pixel space $\lambda$. along the same lines we can define appearance transformations $\phi$, which act on the image itself. the shape should be invariant under change of appearance, conversely, the appearance should be invariant under change of shape. in addition, the shape should transform in the same manner as the image. that means the shape representation is assumed to be equivariant under shape transformations. in summary: \begin{align} \mathbf{f}{\pi(x)} &= \mathbf{f}{x} \tag{invariance of appearance} \ \mathbf{p}{\phi(x)} &= \mathbf{p}x \tag{invariance of shape} \ \mathbf{p}{\pi(x)} &= \pi(\mathbf{p}x) \tag{equivariance of shape} \label{eq:invar} \end{align} % quad \forall i \leq n our method builds on the auto-encoding paradigm, with part shapes and part appearances assuming the role of the latent code. to incorporate these constraints into the loss of an auto-encoder, we reconstruct an image $x$ not from the shape and appearance $(\mathbf{f}x, \mathbf{p}x)$ determined from the original image $x$, but from appropriately transformed images $(\mathbf{f}{\pi(x)}, \mathbf{p}{\phi(x)})$. if the invariance constraints, as formulated above, are fullfilled, these transformations do not change the latent code. thus, the loss implicitly enforces invariance. to obtain shape and appearance, we encode both $\phi(x)$ and $\pi(x)$ with an encoder $\mathrm{e}$. and, after a recombination $\mathrm{r}$ (for details see sec. \ref{sec:architecture}) to a latent image $z$, a decoder $\mathrm{d}$ reconstructs the image. this configuration is depicted in fig. \ref{fig:overview}, the reconstruction loss $\mathcal{l}{\textrm{rec}}$ is as follows: %\begin{align} %\mathcal{l}{\textrm{rec}} &= \lvert x - \mathrm{d}[\mathrm{r}[\mathrm{e}(x)]]\rvert \ %&= \lvert x - \mathrm{d}[\mathrm{r}[ (mathbf{f}x, mathbf{p}x)]]\rvert \nonumber \ %&= \lvert x - \mathrm{d}[\mathrm{r}[ (mathbf{f}{x{\pi'}}, mathbf{p}{x{\phi'}}]]\rvert \nonumber %\end{align} %the to-be-reconstructed image $x{\phi, \pi}$ has been subject to shape and appearance transformations $x \rightarrow \phi \circ \pi (x)$. %we instantiate this loss in a two-stream configuration, as shown in fig. \ref{fig:overview}. %encoder and decoder are the same for each stream, but image representations are crossed. %for simplicity we abbreviate $f = mathbf{f}{\pi(x)}$, $f' = f{\phi(x)}$ , $s = mathbf{p}{\phi(x)}$ and $s' = mathbf{p}{\pi (x)}$. %in accordance with the desired invariances, one can apply a transformation $\phi$ to the image, in order to selectively destroy appearance information in the shape representation and vice versa destroy shape information in the appearance representation with $\pi$. %the first stream encodes $\pi(x)$ to obtain $(a, s')$ and the second stream encodes $\phi(x)$ to obtain $(a', s)$. %then appearance and shape representations are crossed. %after recombining $(a, s)$ to a feature volume $v$ (explained in detail in sec. \ref{sec:architecture}), the decoder reconstructs $x$. \begin{equation} \mathcal{l}{\textrm{rec}}= \lvert x - \mathrm{d}[\mathrm{r}(\mathbf{f}{\pi(x)}, \mathbf{p}{\phi(x)})]\rvert \end{equation} \label{eq:lossrec} %from an information perspective, we apply an appearance transformation to the image, to selectively destroy appearance information in the shape representation and vice versa destroy shape information in the appearance representation with a shape transformation. let us examine what this formulation means on the level of a single part: the part appearance $f^ix$ is extracted at locations in the spatially transformed image $p^i{\pi(x)}$, but then used for reconstruction at the location in the original image $p^i{x}$. for example in fig. \ref{fig:overview} the appearance of the arm will be extracted in a raised position, but then these features are used for reconstructing an arm in a lowered position. for this to succeed, firstly, the appearance features need to be sufficiently abstract. secondly, part locations of the two images have to refer to the same part and track the location of it consistently. this part assignment consistency is an implicit way to improve equivariance under the shape transformations.\ %the shape also need to be invariant under appearance transformations, so part assignment needs to be consistent . %for video data the crossing task can be run on images pairs that show the same object in a different articulation (i.e. different frames of the video), enforcing equivariance of $s$ with respect to natural shape transformations. \ for a known shape transformation the equivariance of shape can also be encouraged explicitly with a loss. this has been used before in the context of unsupervised landmark learning by \cite{thewlis:2017wi, zhang:2018vz} as a point-wise loss on a part probability map, encouraging the exact location of a part to transform accordingly. in our case, the part shapes shall not encode probability, but instead the spatial extend of a part. in approximation, we want the first two moments ($\mu, \sigma$) to transform correctly. thereby the extend and orientation of the parts is penalized in addition to the mere position. \begin{equation} \mathcal{l}{\textrm{equiv}}^i = \mathcal{l}{\mu}^i+ \mathcal{l}{\sigma}^i \label{covariance} \end{equation} the overall loss objective is the sum of the reconstruction loss and the equivariance loss for all $n$ parts: \begin{equation} \mathcal{l} = \sum{i=1}^n \mathcal{l}{\text{equiv}}^i + \mathcal{l}_{\textrm{rec}} \end{equation}


