
\section{Method}

% INTRO FIGURE: PART REPRESENTATION

\begin{figure}[!h]
\begin{subfigure}{0.33\textwidth}
\centering
\includegraphics[trim={0cm 0cm 0cm 0cm},clip, width=.6\linewidth]{fig/rep1_hq}\caption{}
\label{fig:rep1}
\end{subfigure}
\begin{subfigure}{0.33\textwidth}
\centering
\includegraphics[trim={0cm 0cm 0cm 0cm},clip, width=.6\linewidth]{fig/rep2}\caption{}
\label{fig:rep2}
\end{subfigure}
\begin{subfigure}{0.33\textwidth}
\centering
\includegraphics[trim={0cm 0cm 0cm 0cm},clip, width=.6\linewidth]{fig/rep3}\caption{}
\label{fig:rep3}
\end{subfigure}
\caption{ \small{An object is represented in parts. Each part has a distinct spatial localization (part shape) and a corresponding feature descriptor (part appearance). (a) input image, (b) model output of part shapes (each plotted in a different color), (c) schematic illustration of part appearances}}
\label{fig:representation}
\end{figure}

\begin{quotation}
\textit{disentangle as many factors as possible, discarding as little information about the data as is practical} - Y. Bengio, A. Courville and P. Vincent \cite{bengio}
\end{quotation}
To capture an object in an abstract representation, we follow two key ideas: \emph{(i)} disassembling the object into its constituent parts and \emph{(ii)} disentangling spatial geometry (shape) from visual features (appearance). Hence, we model an object as a composition of parts, each part with a part appearance and a part shape, see Fig. \ref{fig:representation}). The part shape should roughly correspond to the area in the image where the part is located, whereas the part appearance is a feature descriptor for that area. The overall object representation is then the collection of part shapes and part appearances. \\
% \begin{equation}
%(a, s) = \cup_i (a_i, s_i)
%\end{equation}
%\label{eq:representation}
% enforcing locality of parts is important
% what do our representations entangle still?:
% shape: position, shape, size
% appearance: illumination, color, texture, material
\todo{local features theme}
The disentanglement of shape and appearance can be enforced by demanding that shape is invariant under the transformation of appearance and vice versa. This is realized in a two-stream auto-encoding formulation. Here, an image is reconstructed from a combination of shape and appearance, with shape extracted from the appearance-transformed image and appearance from a shape-transformed image. Additionally, the part shape is tied to the location of the part in the image: an equivariance loss encourages that the part shape moves in unison with the part in the image. The loss framework is explained in sec. \ref{sec:framework}. \\
To assert a decomposition into independent local parts, we ensure their local modelling and treatment throughout the whole pipeline. This is highlighted when describing the architecture in sec. \ref{sec:architecture}.

\subsection{Framework}\label{sec:framework}

% CROSSING TASK FRAMEWORK
\begin{figure}[h]
\centering
\includegraphics[trim={0cm 0cm 0cm 0cm},clip, width=1.\linewidth]{fig/framework_z}
\caption{ \small{Encoder $E$ encodes shape and appearance for two images $\sigma(X)$ and $\alpha(X)$, after recombination $R$ of $(a_{\sigma(X)}, s_{\alpha(X)})$ into latent image $Z$, the decoder $D$ reconstructs the image $X$.}}
\label{fig:framework}
\end{figure}
%Definitions
We want to represent an object in an image $X \in \mathcal{X} = \{X| X: \Lambda \rightarrow \mathbb{R} \}$, where $\Lambda \subset \mathbb{N}^2$ is the pixel space. Let us denote the shape component of the representation with $s_X$ and the appearance component with $a_X$. For an object with $n$ parts, the overall shape is constituted by the collection of its part shapes $s_X =  \{s^i_X| i=1, ...,  n\}$, the same goes for the appearance $a_X =  \{a^i_X| i=1, ...,  n\}$. We model the part appearances as $k$-dimensional feature vectors $a^i_X \in \mathbb{R}^k$, the part shapes are chosen to be scalar fields like the image itself $s^i_X: \Lambda \rightarrow \mathbb{R}$, similar to a segmentation map. Thereby they enable a direct correspondence of locations in the image to locations in the shape representation.\todo{say the following or not?}\\
%In contrast to other works, where object pose is often modeled by single points as landmarks, a representation based on a scalar field is much richer: whereas a landmarks inform about the exact position, the field-like representation also describes the size and shape of the object parts (see Fig.\ref{fig:representation}).
% Transformations -> Crossing framework
How do we disentangle shape and appearance? In general, a variation in shape will not affect appearance and vice versa. Thus, if we deliberately change shape without changing appearance, we can enforce the invariance of the appearance representation under such a change. We refer to these changes as shape transformations $\sigma: \Lambda \rightarrow \Lambda$, which, if applied to an image $X$, directly act on the underlying pixel space $\Lambda$. Along the same lines we can define appearance transformations $\alpha: \mathcal{X \rightarrow \mathcal{X}}$.
The shape should be invariant under change of appearance, conversely, the appearance should be invariant under change of shape. In addition, the shape should transform in the same manner as the image. That means the shape representation is assumed to be equivariant under shape transformations. In summary:
\begin{align}
 a_{\sigma(X)}  &= a_{X} \tag{invariance of appearance}\\
  s_{\alpha(X)} &= s_X  \tag{invariance of shape}\\
  s_{\sigma(X)} &= \sigma(s_X) \tag{equivariance of shape}
\label{eq:invar}
\end{align} % quad \forall i \leq n
To incorporate these constraints into the loss of an auto-encoder, we reconstruct an image $X$ not from the shape and appearance determined from the original image $(a_X, s_X)$, but from appropriately transformed images $(a_{\sigma(X)}, s_{\alpha(X)})$. If the invariance constraints, as formulated above, are fullfilled, these transformations do not change the representation. To obtain shape and appearance, we encode both $\alpha(X)$ and $\sigma(X)$ with an encoder $\mathrm{E}$. And, after a recombination $\mathrm{R}$ (for details see sec. \ref{sec:architecture}) to a latent image $Z$, a decoder $\mathrm{D}$ reconstructs the image. This configuration is depicted in Fig. \ref{fig:framework}, the reconstruction loss $\mathcal{L}_{\textrm{rec}}$ is as follows:
%\begin{align}
%\mathcal{L}_{\textrm{rec}} &=  \lVert X -  \mathrm{D}[\mathrm{R}[\mathrm{E}(X)]]\rVert \\
%&= \lVert  X  - \mathrm{D}[\mathrm{R}[ (a_X, s_X)]]\rVert \nonumber \\
%&= \lVert  X  - \mathrm{D}[\mathrm{R}[ (a_{X_{\sigma'}}, s_{X_{\alpha'}}]]\rVert \nonumber
%\end{align}
%The to-be-reconstructed image $X_{\alpha, \sigma}$ has been subject to shape and appearance transformations $X \rightarrow \alpha \circ \sigma (X)$.
%We instantiate this loss in a two-stream configuration, as shown in Fig. \ref{fig:framework}. %Encoder and decoder are the same for each stream, but image representations are crossed.
%For simplicity we abbreviate $a = a_{\sigma(X)}$, $a' = a_{\alpha(X)}$ , $s = s_{\alpha(X)}$ and $s' = s_{\sigma (X)}$.
%In accordance with the desired invariances, one can apply a transformation $\alpha$ to the image, in order to selectively destroy appearance information in the shape representation and vice versa destroy shape information in the appearance representation with $\sigma$.
%The first stream encodes $\sigma(X)$ to obtain $(a, s')$  and the second stream encodes $\alpha(X)$ to obtain $(a', s)$.
%Then appearance and shape representations are crossed.
%After recombining $(a, s)$ to a feature volume $V$ (explained in detail in sec. \ref{sec:architecture}), the decoder reconstructs $X$.
\begin{equation}
\mathcal{L}_{\textrm{rec}}= \lVert  X  - \mathrm{D}[\mathrm{R}(a_{\sigma(X)}, s_{\alpha(X)})]\rVert
\end{equation}
\label{eq:loss_rec}
%From an information perspective, we apply an appearance transformation to the image, to selectively destroy appearance information in the shape representation and vice versa destroy shape information in the appearance representation with a shape transformation.
Let us examine what this formulation means on the level of a single part: the part appearance $a^i_X$ is extracted at locations in the spatially transformed image $s^i_{\sigma(X)}$, but then used for reconstruction at the location in the original image $s^i_{X}$. For example in Fig.  \ref{fig:framework} the appearance of the arm will be extracted in a raised position, but then these features are used of reconstruct an arm in a lowered position. For this to work, firstly, the appearance features need to be sufficiently abstract. Secondly, part locations of the two images have to refer to the same part and track the location of it consistently. This enforces equivariance under the shape transformation, which connects the two images, implicitly.\\
%The shape also need to be invariant under appearance transformations, so part assignment needs to be consistent .
%For video data the crossing task can be run on images pairs that show the same object in a different articulation (i.e. different frames of the video), enforcing equivariance of $s$ with respect to natural shape transformations. \\
For a known shape transformation the equivariance of shape can also be encouraged explicitly with a loss. This has been used before in the context of unsupervised landmark learning by \cite{thewlis17, Zhang18} as a point-wise loss on a part probability map, encouraging the exact location of a part to transform accordingly. In our case, the part shapes shall not encode probability, but instead the localization of a part. In approximation, we want the first two moments to transform correctly. Thereby the extend and orientation of the parts is penalized in addition to the mere position.
\begin{equation}
\mathcal{L}_{\textrm{equiv}}^i = \mathcal{L}_{\mu}^i+ \mathcal{L}_{\Sigma}^i = \lVert \mu_{\sigma}^i - \mu_{\sigma'}^i \rVert_{2}    + \lVert \Sigma_{\sigma}^i - \Sigma_{\sigma'}^i  \rVert_{1}
\label{covariance}
\end{equation}
%= \lVert \mu_{\sigma}^i - \mu_{\sigma'}^i \rVert_{2}  + \lVert \Sigma_{\sigma}^i - \Sigma_{\sigma'}^i \rVert_{1}
%We calculate the moments for an image under two different shape transformations $\sigma$, $\sigma'$. The moments are w.r.t. the pixels in the original image's pixel space. For details, please refer to the appendix. \todo{is it possible to refer to the appendix}
We calculate the moments w.r.t. the pixels $p$ in the original image's pixel space under two different shape transformations $\sigma, \sigma'$. To account for density distortions due to the shape transformations we also consider the discrete approximation to the Jacobian determinant of the shape transformation $D = \text{det}(J_{\sigma}(p))$. The moments are then calculated as
%\begin{align*}
%\mu_{\sigma}^i &= \frac{1}{N} \sum_{p} D \, s^i_{\sigma(X)}(p) * \sigma(p) \\ %\nonumber
%\todo{actually this is also augmented with appearance transform}
%\Sigma_{\sigma}^i &= \frac{1}{N} \sum_{p} D \, s^i_{\sigma(X)}(p) * (\sigma(p) - \mu_\sigma^i) (\sigma(p) - \mu_{\sigma}^i)^T  %\nonumber %\mathrm{d}p\\
%\end{align*}
\begin{equation}
\mu_{\sigma}^i = \frac{1}{N} \sum_{p} D \, s^i_{\sigma(X)}(p) * \sigma(p) \mathrm{,}\,\,\, \Sigma_{\sigma}^i = \frac{1}{N} \sum_{p} D \, s^i_{\sigma(X)}(p) * (\sigma(p) - \mu_\sigma^i) (\sigma(p) - \mu_{\sigma}^i)^T  \nonumber %\mathrm{d}p\\
\end{equation}
, where the normalization is $N = \sum_{p} D \, s^i_{\sigma(X)}(p)$. The overall loss objective is the sum of the reconstruction loss and the equivariance loss for all $n$ parts:
\begin{equation}
\mathcal{L} = \sum_{i=1}^n \mathcal{L}_{\text{equiv}}^i + \mathcal{L}_{\textrm{rec}}
\end{equation}


\subsection{Architecture}\label{sec:architecture}
The auto-encoding pipeline consists of three stages, namely: \textbf{encoding} both shape and appearance for each part,  \textbf{recombining} this information meaningfully into a latent image and \textbf{decoding} this latent image to reconstruct the image. The whole process is sketched in Fig. \ref{fig:framework}, the operations in more detail are visualized in Fig. \ref{fig:encoding}. Throughout the procedure we maintain the local correspondence between the representation and the image: We ensure a local appearance extraction in the encoding, a local synthesis in the recombining and a local usage of the latent image in the decoding. These architectural restrictions enable a disentangled part representation with the interpretation of a part as a localized entity.
\paragraph{Encoding}
% ENCODING
\begin{figure}[!h]
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[trim={0cm 0cm 0cm 0cm},clip, width=1.\linewidth]{fig/x1}\caption{}
\label{fig:x1}
\end{subfigure}
\hspace{2cm}
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[trim={0cm 0cm 0cm 0cm},clip, width=.5\linewidth]{fig/x2}\caption{}
\label{fig:x2}
\end{subfigure}
%\begin{subfigure}{0.28\textwidth}
%\centering
%\includegraphics[trim={0cm 0cm 0cm 0cm},clip, width=1.\linewidth]{fig/recombine_crop}\caption{}
%\label{fig:encode1}
%\end{subfigure}
\caption{ \small{Encoding: (a) Given an input image an hourglass network predicts part shapes, a second shallow hourglass network receives both input image and the part shapes and outputs a feature stack.
(b) The feature stack is folded with the part shapes to yield a set of part appearances}}
\label{fig:encoding}
\end{figure}
 ${(a, s | X)}$\footnote{ For a slim notation, we leave out the explicit reference to the generic input image $X$ in this section: $a, s, a^i, s^i$ refer to $a_X, s_X, a^i_X, s^i_X$.} The encoding of shape and appearance given an image proceeds in two steps: \\
\emph{(i.)} $(s|X)$: The part shapes are predicted given the image. To extract part shapes we use an hourglass\footnote{ We utilize hourglass neural network models in both steps, as these models are able to preserve pixel-wise locality, but integrate information from multiple scales \cite{hourglass}.}  neural network: The input is an image $X$, the output a stack of $n$ part shapes $s =  \{s^i| i=1, ...,  n\}$.\\ \emph{(ii.)} $(a|s,X)$: The part appearances $a =  \{a^i| i=1, ...,  n\}$ are predicted given the image and the part shapes. Again we use an hourglass network, albeit shallower. The input is the original image concatenated with the stack of part shapes. The output is a feature stack $A$. A part appearance is obtained by folding the feature stack with the a part shape: $a^i = \sum_{p \in \Lambda} A(p) s^i(p) $. Each $a^i$ now describes the appearance of the neighborhood of a part spatially localized by the part shape $s^i$.
\paragraph{Recombining}
In the analysis-by-synthesis regime, once the object representation is successfully factorized, one can make assumptions how the factors reunite to generate an image, following the knowledge and intuition about how objects give rise to images in the physical world.
Firstly, we remerge shape and appearance into images of descriptors at the correct locations. For each part, appearance is convolved with the corresponding shape, yielding $n$ part feature images: $z^i(p) = a^i * s^i(p)$.
Secondly, we reassemble the object from its parts: the part feature images $z^i$ are summarized by summing in a single image: $ Z(p) = \sum_i \frac{z^i(p)}{1 + \sum_j z^j(p)}$. The result is an image of part feature descriptors located according to their corresponding part shape, which we call latent image $Z$.

% DECODING
\begin{figure}[!h]
\begin{subfigure}{0.5\linewidth}
\centering
\includegraphics[trim={0cm 0cm 0cm 0cm},clip, width=1.\linewidth]{fig/x3}\caption{}
\label{fig:x3}
\end{subfigure}
%\hspace{4cm}
\begin{subfigure}{0.4\linewidth}
\centering
\includegraphics[trim={0cm 0cm 0cm 0cm},clip, width=1.\linewidth]{fig/x4}\caption{}
\label{fig:x4}
\end{subfigure}
%\begin{subfigure}{0.28\textwidth}
%\centering
%\includegraphics[trim={0cm 0cm 0cm 0cm},clip, width=1.\linewidth]{fig/recombine_crop}\caption{}
%\label{fig:encode1}
%\end{subfigure}
\caption{ \small{(a) Recombining part shapes $s_{\alpha(X)}$ and part appearances $a_{\sigma(X)}$ to a latent image $Z$.
(b) Decoding latent image $Z$ to reconstruction target image $X$.}}
\label{fig:decoding}
\end{figure}

\paragraph{Decoding}
Finally, the latent image needs to be decoded to an image. This is done by a neural network decoder. The decoder architecture is modeled after an hourglass-half. The latent image is scaled to different scales and inserted, after each layer, in addition to the part shapes\todo{why add shapes? comment}. As before, the crucial property of the parts that needs to be conserved is their local direct correspondence to the image. In the decoding process, one needs to consider locality twice: On the one hand one needs to assure, that the receptive field of the neurons does not extend to the full image, in order to thwart a complex non-local interaction of part information.
On the other hand, it is essential to regularize the information already before passing it to the decoder. Keeping in mind that the part shape should be of rather simple geometry, we introduce two differentiable information bottlenecks, in order to prevent the shape from being scattered over the object.
One bottleneck is less restrictive than the other. It is an approximation of the part shape as $\hat s^i(p) = \frac{1}{1 + (p -\mu)^T \Sigma^{-1} (p - \mu)}$, where $\mu$ and $\Sigma$ are the mean and the covariance matrix of the part shape $s^i$. This allows to pass second-order information such as size and orientation in addition to the position of the part to the decoder.
For the other bottleneck, we use the same approximation, but fix the covariance $\Sigma$ to the identity matrix. Hence, effectively only information about the position of each part shape can reach the decoder. Note that these operations are fully differentiable.

\subsection{Implementation details}\label{implementationdetails}
\begin{itemize}
\item The image resolution is $\Lambda_X = 128 \times 128$, but the resolution of corresponding part shapes is $\Lambda_s = 64 \times 64$.
\item For the reconstruction  loss $\mathcal{L}_\text{rec}$ we use the $L_1$ or $L_2$ distance.
\item  To prevent parts from trying to explain the whole image, instead of focusing on the object, we also restrict the reconstruction loss to an area around the part shape: a sum of Gaussian approximations around the means of the part shapes is folded with the loss.
\item To instantiate shape transformations $\sigma$, one needs image pairs that show the same object in a different articulation or position: For static images an artificial thin-plate spline transform (TPS) can be applied, which generalizes rotation, scaling, translation. For video data adjacent frames exhibit natural shape transformations.
\item The appearance transformation $\alpha$ is encompassing a colour augmentation, contrast variations, and changes in brightness. In general, the more selective the transformation distinguishes shape and appearance, the more invariant the representation.

\end{itemize}

