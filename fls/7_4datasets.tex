\section*{CelebA}{CelebA} \cite{liu15facewild} contains ca. 200k celebrity faces of 10k identities.
We resize all images to $128\times 128$ and exclude the training and test set of the MAFL subset, following \cite{thewlis17}.
As  \cite{thewlis17, zhang18}, we train the regression (to 5 ground truth landmarks) on the MAFL training set (19k images) and test on the MAFL test set (1k images).

\section*{Cat Head}{Cat Head} \cite{zhang08cathead}  has nearly 9k images of cat heads.
We use the train-test split of \cite{zhang18} for training (7,747 images) and testing (1,257 images).
We regress 5 of the 7 (same as \cite{zhang18}) annotated landmarks.
The images are cropped by bounding boxes constructed around the mean of the ground truth landmark coordinates and resized to $128\times128$.


\section*{CUB-200-2011}{CUB-200-2011} \cite{wah11birds} comprises ca. 12k images of birds in the wild from 200 bird species.
We excluded bird species of seabirds, roughly cropped using the provided landmarks as bounding box information and resized to $128\times128$.
We aligned the parity with the information about the visibility of the eye landmark.
For comparing with \cite{zhang18} we used their published code.


\section*{BBC Pose}{BBC Pose} \cite{charles13bbcpose} contains videos of sign-language signers with varied appearance in front of a changing background. Like \cite{jakab18} we loosely crop around the signers.
The test set includes 1000 frames and the test set signers did not appear in the train set.
For evaluation, as \cite{jakab18}, we utilized the provided evaluation script, which measures the PCK around $d=6$ pixels in the original image resolution.


\section*{Human3.6M}{Human3.6M} \cite{ionescu14human36m} features human activity videos.
We adopt the training and evaluation procedure of \cite{zhang18}.
For proper comparison to \cite{zhang18} we also removed the background using the off-the-shelf unsupervised background subtraction method provided in the dataset.


\section*{Penn Action}{Penn Action} \cite{zhang13penn} contains 2326 video sequences of 15 different sports categories.
For this experiment we use 6 categories (tennis serve, tennis forehand, baseball pitch, baseball swing, jumping jacks, golf swing).
We roughly cropped the images around the person, using the provided bounding boxes, then resized to $128\times128$.


\section*{Dogs Run}{Dogs Run} is made from dog videos from YouTube totaling in 1250 images under similar conditions as in Penn Action. The dogs are running in one direction in front of varying backgrounds. The 17 different dog breeds exhibit widely varying appearances.


\section*{Deep Fashion}{Deep Fashion} \cite{liu16deepfashion, liu16deepfashionwild} consists of ca. 53k in-shop clothes images in high-resolution of $256 \times 256$. We selected the images which are showing a full body (all keypoints visible, measured with the pose estimator by \cite{cao17affinityfield}) and used the provided train-test split.
For comparison with Esser \etal \cite{esser18} we used their published code.


