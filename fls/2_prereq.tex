%&tex
\chapter{Prerequisites on Learning Disentanglement}\label{sec:prerequisites}

\section{Learning from Data}
	{Learning from data} is commonly understood as the ability of algorithms to improve their performance on a task with experience accumulated from the observation of data \cite{goodfellow16dlb}. The source of data is usually a dataset - set of data points $X = \{x_i | i \in \{1\ldots n\} \}$, which are sampled from a probability distribution $x_i \sim p(x)$.
	In general, these data points are multi-dimensional. In computer vision in particular, data are images $\mathbf{x}$ with height $h$ and width $w$, so that the dimension is $\mathbf{x} \in \mathbb{R}^{h \times w}$.

	\subsection{Supervised}\label{sec:supervised}
		The term {supervised learning} denotes the task to learn a mapping from data points $x_i$ to target labels $y_i$.
		A supervised algorithm has access to data-label pairs  $(y_i, x_i) \sim p(y, x)$, in order to estimate the connection between data points and labels, either in form of a conditional probability $p(y|x)$, or in form of a deterministic function $y = f(x)$.
		The label $y$ can be either discrete (\eg information about an object class) or continuous (\eg the location of an object part in an image).
		Recent advances, in particular the effectiveness of neural network models (cf. sec. \ref{sec:neuralnetworks}) on big datasets, have led to huge progress on problems that can be formulated as regression or classification. That is why on many traditional computer vision problems, such as \eg object recognition, image classification or human pose estimation, machines are now performing on a superhuman level; hence, these problems are now considered to be essentially solved.\\
		The Achilles' heel of supervised learning lies in the need for a viable supervision signal. To get labels, it is usually required to manually annotate the data. The human effort in this is costly, error-prone and not scalable to the ever-growing vast amounts of raw data.

	\subsection{Unsupervised}\label{sec:unsupervised}
		{Unsupervised learning} is the endeavour to learn about structures and patterns in unlabelled data. In this paradigm, the learning algorithm has access to the samples of the data distribution $x \sim p(x)$. The task is usually framed as a form of density estimation, \ie to model the entire distribution in a probabilistic generative model (cf. sec. \ref{sec:genmodel}).
		Unsupervised learning is considered much harder than supervised learning \cite{bishop06pattern}. There are several complications in the design of unsupervised algorithms:
		\begin{itemize}
			\item Naturally, without supervision, \textit{the goal of learning is not specified}, hence surrogate objectives have to be formulated. The lack of specification renders the evaluation often arbitrary and subjective~\cite{theis15evalgen}.
			\item It is a priori not clear, \textit{how much prior knowledge should be embedded}. To introduce no artificial bias, some argue for a purely data-driven approach. Others argue for the importance of certain inductive priors to guide learning \cite{tenenbaum18think}.
			A related modeling choice is, if the algorithm should be model-free or model-based\todo{ cite huszar article}.
			\note{model-free vs model-based approaches:}
			\note{model-based $\rightarrow$ more flexible, transferable, allows for modular combination (like parts)}
			{In this work we argue for using more prior knowledge and modeling assumptions to obtain strong constraints.}
				\todo{it is not clearly defined: what is unsupervised?}

		\end{itemize}
		\todo{connection between unsupervised and supervised learning, \cite{goodfellow16dlb}.}
		\note{A possible framing of the goal is data compression.}
		\note{e.g. outlier detection where $p(x)$ has low probability}
		\note{What does unsupervised even mean? No prior assumptions, no knowledge at all? Unspecified.. read on this.}
		\note{notion of truly unsupervised learning is actually harmful to progress.}

	\subsection{Artificial Neural Networks}\label{sec:neuralnetworks}
		Artificial neural networks are a powerful and flexible tool for function approximation. In their principles they are inspired by biological neurophysiology. An artificial network is a model for a function $y = f(x)$ with vector input $x = \{ x_i | i = 1 \ldots n \}$ and vector output $y = \{ y_j | j = 1 \ldots m \}$:
		\begin{equation} \label{eq1}
			\begin{split}
				h_j & =  a (\sum_i w_{ji} x_i + b_i)  \\
				y_j & =  a' (\sum_i w'_{ji} h_i + b'_i)
			\end{split}
		\end{equation},
		with weight matrices $w, w'$, non-linear so-called activation functions $a, a'$ (\eg $a(x)=0$ for $x<0$, $a(x)=x$ for $x>=0$) and bias vectors $b, b'$.
		The components $h_j$ are called hidden units or neurons. Neural networks can also comprise multiple hidden layers via $h_j  =  a (\sum_i w_{ji} h_i + b_i)$.
		It can be shown, that in the limit of infinite hidden units $h_j$ they can approximate any (continuous) function arbitrarily close \cite{cybenko89approx, hornik91approx}.
		In practice, however, networks with more that one layer, referred to as deep neural networks, seem to work better. This may be due to the possibility of building a hierarchical feature representation \cite{zeiler14vis}.

		For processing image data, the weight matrices can be constrained to be only locally connected and to share weights across locations to enforce translation invariance, resulting in \textit{convolutional} neural networks.
		To optimize these weights via gradient descent has proven successful (for deep networks this is called backpropagation).
		\note{Need to be differentiable, (differentiable programming).}

\section{Generative Models}\label{sec:genmodel}
	\begin{quote}
	    What I cannot create, I do not understand. - R. Feynman
	\end{quote}
	Learning and understanding structure in data by being able to generate, is the rationale behind generative modelling.
	Generative models are mostly applied for unsupervised learning and can be contrasted to discriminative models. While discriminative models are used to model posterior conditionals $p(y|x)$ (\eg for supervised learning (cf. sec. \ref{sec:supervised}), generative models capture the complete data distribution $p(x)$ in an estimate $\hat p(x)$ \cite{bishop06pattern}. Thus, after estimation, one can generate samples from this model $\hat p$. Hence the name generative model.
	\note{Generative modeling can be used for outlier search, where regions with low probability under the model are taken as indicative for an outlier.}
	The currently predominant generative models are built on either autoencoding or adversarial formulations:

	\subsection{Autoencoding Formulations}\label{sec:autoencoding}
		An autoencoding model is learning by reconstructing samples of data, $\hat x = f(x)$. To enforce data compression (otherwise the identity function is a trivial solution of autoencoding) the function has an information bottleneck, namely an inferred latent code $z$ of reduced dimension. The autoencoder is then the chain of an encoding function $z = e(x)$ and a decoding function $\hat x = d(z) = d(e(x))$.

		Whereas the conventional autoencoder consists of deterministic mappings $e, d$, the \textit{variational autoencoder} \cite{kingma13vae} models the probability distribution $p(x)$. More specifically, it maximizes a lower bound to the logarithmic likelihood $\log p(x)$ of data $x$. This so-called variational lower bound $\mathcal{L}$ is given by:
		% \begin{equation}\label{eq:vae}
			% \mathcal{L} = \underbrace{\mathds{E}_{z\sim q(z|x)}  \log p(x|z)}_{\textrm{reconstruction loss}}  - \underbrace{\textrm{KL}(q(z|x)||p(z))}_{\textrm{regularization}}
		% \end{equation}
		\begin{equation}\label{eq:vae}
			\mathcal{L} = \mathds{E}_{z\sim q(z|x)}  \log p(x|z) - \textrm{KL}(q(z|x)||p(z))
		\end{equation}

		Where $z$ introduces latent variables, with a prior distribution $p(z)$. The approximation to the posterior $q(z|x)$ of the latent variables and the posterior of the data given the latent variables $p(x|z)$. If one wants to model the distributions with neural networks, one typically uses Gaussian distributions and lets the networks predict the parameters (mean $\mu$ and variance $\Sigma$) based on the image.
		In the current machine learning contexts, all functions ($e, d$) and or moments ($\mu, \Sigma$) are modelled with neural networks.

	\subsection{Adversarial Formulations}\label{sec:adversarial}
		\textit{Generative adversarial networks} (GAN) \cite{goodfellow14gan} consist of two neural networks competing in a zero-sum game. A generator network $G$ is generating images based on a latent code $z$ sampled from a distribution $p(z)$. The discriminator network $D$ is a binary classifier with the task to classify an image as originating from the data distribution $p_{\mathrm{data}}$ or from the distribution produced by $G$. The loss function of $G$ is the negative of the loss of $D$, such that one can formulate the optimization in a minmax form:
		\begin{equation}
			% \begin{split}
			\min_D \max_G - \frac{1}{2} \mathds{E}_{x \sim p_{\mathrm{data}}} [\log D(x)] - \frac{1}{2} \mathds{E}_{z\sim p(z)}[\log (1-D(G(z)))]
			% \end{split}
		\end{equation}
		The generator is then optimized to make the output indiscriminable from the data distribution.
		The discriminator can be interpreted as a learned similarity metric, to measure the closeness of an image to the data distribution \cite{larsen15vaegan}.
		There are many variants and extensions to this basic principle of learning with an adversarial task. For example, one can learn a discriminator on for a set of image patches \cite{isola17image2image}. \todo{find more examples, of the gan zoo}

\section{Disentangling Representations}\label{sec:disentangled}
	In supervised learning, a performance measure is naturally induced by the metric, that is being optimized. In the unsupervised setting, judging the performance of a model is less straightforward.
	How to rate the quality of the latent representation?
	\note{For example, when modelling an image domain, one could subjectively rate the quality of the generated image.}
	\todo{introduce latent representation in data compression framing }

	\subsection{Learning Representations}

	\begin{quote}
		{Disentangle as many factors as possible, discarding as little information about the data as is practical.} - Bengio \etal \cite{bengio13rep} % Y. Bengio, A. Courville and P. Vincent \cite{Bengio2013rep}
	\end{quote}

	According to Bengio \etal \cite{bengio13rep},  a representation is useful, if it can be applied to many - in advance unknown - different tasks, while being trained on only one particular task.
	As the downstream tasks can be multifarious, the essential \textit{information} should be contained in the representation.
	For some tasks only a subset of aspects of the data will be necessary, that is why \textit{disentangled factors} make a representation particularly practical.\\
	The latent representation $z$ learned by generative models captures the essential \textit{information} of the data distribution. That is made sure by requiring the ability to generate samples from the original data distribution from it.
	How then to reach the second goal, the \textit{disentanglement} of generative factors?

	\subsection{Disentangling by Equivariance and Invariance}
		What is a factor? As outlined in the introduction (cf. sec. \ref{sec:introduction}), factors in a representation should correspond to causal elements of the world.
		In general, these factors can interact in complicated ways to finally result in an image. Here, we only consider the case where multiple independent factors each have an influence (cf. Fig. \ref{fig:infer}).
		A change in an element, should then lead to: \emph{i)} a corresponding change in the representational factor and \emph{ii)} leave other factors, that represent other elements, unchanged.
		Formally, this can be seen as inference: a number of latent variables $\mathbf{z_1}\ldots\mathbf{z_N}$ interacted to cause the existence of the observed image $\mathbf{x}$. The task is now to infer estimates for these latent variables $\mathbf{\hat z_i}$. A graphical model of the process is shown in figure \ref{fig:infer}.
		A disentangled representation should simultaneously fulfill equivariance and invariance: A change in $\mathbf{z_i}$ should: \emph{i)} \textit{equivariantly} change in the abstract representational factor $\mathbf{\hat z_i}$, \emph{ii)} while leaving the other factors $\mathbf{\hat z_j}, j\neq i$, that represent other causes, \textit{invariant}.
		\begin{figure}[ht]
			\centering
			\input{fls/tikz/inferlevel.tex}
			\caption{Disentangling causal factors means to infer an estimate - \ie a representation - from an image}
			\label{fig:infer}
		\end{figure}
		\todo{mathematically,.. $f \circ g (x) = ... $}
		\todo{draw other: arbitrary causal}


\section{Theoretical Impediments from Causality}\label{sec:causality}
	Generative factors represent causal elements.
	Learning a disentangled representation of generative factors is then understood as causal inference.
	In accordance with the causal literature, we can make statements about the type of knowledge, that can be gained by the type of data provided. It turns out that from "raw" image data, it is actually impossible learn a disentangled representation $z$ - raw data referring to images $x$ sampled from $p(x)$, without further assumptions.
	To elucidate this fact, we start with a primer for causal learning (sec. \ref{sec:causallearning}), outline which inductive biases are needed for disentanglement (sec. \ref{sec:requirements}) and assess how one can instantiate such biases for disentangling the factors of shape and appearance in images (sec. \ref{sec:transform}, sec. \ref{sec:anabysyn})).

	\subsection{Causal Learning}\label{sec:causallearning}
		\begin{figure}[t]
			\begin{subfigure}{0.3\linewidth}
				\centering
				\input{fls/tikz/causes.tex}
				\caption{}
			\end{subfigure}
			\begin{subfigure}{0.3\linewidth}
				\centering
				\input{fls/tikz/caused.tex}
				\caption{}
			\end{subfigure}
			\begin{subfigure}{0.3\linewidth}
				\centering
				\input{fls/tikz/latentcause.tex}
				\caption{}
			\end{subfigure}
			\caption{Correlation implies causation - if $x_1$ and $x_2$ correlate, a) $x_1$ may cause $x_2$,  b) $x_1$ may be caused by $x_2$ or c) both are contingent on a latent cause $z$}
			\label{fig:reichenbach}
		\end{figure}

		Learning to infer causality is harder than statistical learning. We outline the basic problem for the case of two variables $x_1, x_2$: statistical learning aims at estimating probabilistic properties such as $p(x_1, x_2)$ or  $p(x_2|x_1)$ from data.
		It is a well-known theme in statistics is that correlation does not imply causation. Less well-known\todo{formulate differently} is Reichenbachs principle \cite{peters17elements, reichenbach56time}, that states: if two random variables are statistically dependent, then there exists a third variable that influences both or a direct causal link between them (Fig. \ref{fig:reichenbach}).
		In addition to estimating the probability distribution, also the causal structure has to be inferred \cite{peters17elements}.

		We start with an intuitive example problem (adapted from \cite{pearl18why}):
		How to learn the causal connection between a barometer and the weather? If the barometer is working well, there exists a clear correlation between the weather condition and the needle position. Given a dataset showing both barometer and corresponding weather condition, a capable machine learning algorithm will be able to capture this correlation. However, it will fail to understand the causal direction, since this is not possible from the data.
		Imagine how a human would go about solving this problem:
		Having a mechanistic model of the world he could reason about the precise causal mechanism relating weather to humidity to needle position. A simple model could be: weather influences air pressure, pressure influences barometer needle position.
		What if one has no prior knowledge? A solution of childlevel simplicity is, to force the needle to move with a finger. Without the power of voodoo magic, the weather will not change. Hence causality has to go other way or via a third latent variable influencing both \ie air pressure.
		% There cannot be an abstract intelligence, which finds out about the world purely by observation. The intelligence has to interact with the world, it has to be in the world.
		% e.g.  RCT
		To conclude, the strength of association (correlation) can be estimated with observational data alone, this can answer the question: how likely will it rain, if the barometer needle sinks? But not: how would the weather change if I force the barometer needle to sink?

		Pearl \cite{pearl18why} distinguishes between three types of questions, that can be answered by different types of knowledge:
		\begin{enumerate}
			\item Association. What if I see \ldots?
			\item Intervention. What if I do \ldots?
			\item Counterfactual. What if I had done \ldots?
			%Particularly problematic to learn from data: data consist of facts, not counterfacts.
		\end{enumerate}
		The levels of this \textit{ladder of causation} \cite{pearl18why} are separate not only conceptually, but in the type of data or assumptions that have to be made in order to access them. In particular, by unsupervised learning from observational data only the first level is accessible. The second level requires interactional data or model assumptions, while the third is inaccessible without an explicit model. The answers to these hypothetical questions (counterfactuals) lie by definition not in the data (facts).


	\subsection{Disentangling requires Interventions or Model Assumptions}\label{sec:requirements}
		The results from the study of causal inference also entail that "purely" unsupervised disentangling, \ie estimating $\mathbf{\hat z_i}$ from samples $x \sim p(x)$, is impossible. A proof for this can be found in \cite{locatello18challenging}.
		% so far fitting curve p(x) to data manifold
		Current machine learning operates mostly on the level of association, estimating (complex) correlations from raw data.
		As we have seen, this purely data-driven approach can only go so far.
		In contrast, humans seem to have the ability to interact with their environment and have innate assumptions on coherence, causality, physics etc., which introduce inductive priors~\cite{tenenbaum18think}.
		To bring \emph{i)} interventions and \emph{ii)} model assumptions to our problem of disentangling shape and appearance, we \emph{i)} apply changes to an image, which are assumed to change only one factor and \emph{ii)} model the causal process of the image generation in the theme of analysis-by-synthesis.
		\todo{math}
		% measure: p(x)
		% assume causal model: p(x | a, s)
		% want: p(s | x) and p(a | x)
%
% encoding
% $p(s | x )$
% $p(a | x) = p(a | s, x) p(s | x)$
%
% decoding
% $p(x) = p(x | a, s) p(a) p(s)$
%
% $p(x| do(s), do(a))$

	\subsection{Image Transformation as Intervention}\label{sec:transform}
		\begin{figure}[htp]
			\centering
			\input{fls/tikz/trans.tex}
			\caption{An image $\mathbf{x}$ is assumed to be generated from the factors of shape $\sigma$ and appearance $\alpha$. Implementing an intervention with a transformation of factors, means changeing one factor without changing the other.}
			\label{fig:intervene}
		\end{figure}
		% $p(x| do(a), b)$
		To disentangle shape $\sigma$ and appearance $\alpha$, we can emulate interventions by image transformations. Under the assumption that certain transformations only lead to a change in shape, while leaving appearance invariant or vice versa, we obtain access to interventional data. %of the type x | s, a , x | do(s), a

	\subsection{Analysis-by-Synthesis to Model Assumptions}\label{sec:anabysyn}
		Additional assumptions can be made about the image generation process in the regime of analysis-by-synthesis:
		The key idea is, that the process of how an image is generated from underlying factors (graphics), is much better known that estimating the factors from the image (inverse graphics)\todo{ look up definition for inverse graphics}.
		One can combine a model for analysis and a model for synthesis to reconstruct an image (autoencoding, cf. sec \ref{sec:autoencoding}).
		Herein the synthesis can be tightly constrained to fit the assumptions about reality \cite{tieleman14thesis}.
		Assumptions about how shape and appearance interact enable disentanglement.

\todo{add shape and appearance}
\todo{objects}
