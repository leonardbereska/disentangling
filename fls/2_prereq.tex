%&tex
\chapter{Prerequisites on Learning Disentanglement}\label{sec:prerequisites}
\begin{comment}
\section{Learning from Data}
	{Learning from data} is commonly understood as the ability of algorithms to improve their performance on a task with experience accumulated from the observation of data \cite{goodfellow16dlb}. The source of data is usually a dataset - set of data points $X = \{x_i | i \in \{1\ldots n\} \}$, which are sampled from a probability distribution $x_i \sim p(x)$.

	\subsection{Supervised}\label{sec:supervised}
		The term {supervised learning} denotes the task to learn a mapping from data points $x_i$ to target labels $y_i$.
		A supervised algorithm has access to data-label pairs  $(y_i, x_i) \sim p(y, x)$, in order to estimate the connection between data points and labels, either in form of a conditional probability $p(y|x)$, or in form of a deterministic function $y = f(x)$.
		The label $y$ can be either discrete (\eg information about an object class) or continuous (\eg the location of an object part in an image).
		Recent advances, in particular the effectiveness of neural network models (cf. sec. \ref{sec:neuralnetworks}) on big datasets, have led to huge progress on problems that can be formulated as regression or classification. That is why on many traditional computer vision problems, such as \eg object recognition, image classification or human pose estimation, machines are now performing on a superhuman level; hence, these problems are now considered to be essentially solved.\\
		The Achilles' heel of supervised learning lies in the need for a viable supervision signal. To get labels, it is usually required to manually annotate the data. The human effort in this is costly, error-prone and not scalable to the ever-growing vast amounts of raw data.

	\subsection{Unsupervised}\label{sec:unsupervised}
		{Unsupervised learning} is the endeavour to learn about structures and patterns in unlabelled data. In this paradigm, the learning algorithm has access to the samples of the data distribution $x \sim p(x)$. The task is usually framed as a form of density estimation, \ie to model the entire distribution in a probabilistic generative model (cf. sec. \ref{sec:genmodel}).
		Unsupervised learning is considered much harder than supervised learning. There are several complications in the design of unsupervised algorithms:
		\begin{itemize}
			\item Naturally, without supervision, the goal of learning is not specified, hence surrogate objectives have to be formulated. The lack of specification renders the evaluation often arbitrary and subjective\todo{cite evaluation of generative models}.
			\item It is a priori not clear, how much a priori knowledge should be embedded. To introduce no artificial bias, some argue for a purely data-driven approach. Others argue for the importance of certain inductive priors to guide learning \cite{tenenbaum18think}.
			A related modeling choice is, if the algorithm should be model-free or model-based\todo{ cite huszar article}.
			\note{model-free vs model-based approaches:}
			\note{model-based $\rightarrow$ more flexible, transferable, allows for modular combination (like parts)}


			\note{In this work we argue for using more prior knowledge and modeling assumptions to obtain strong constraints.}

		\end{itemize}
		\todo{connection between unsupervised and supervised learning, \cite{goodfellow16dlb}.}
		\note{A possible framing of the goal is data compression.}
		\note{e.g. outlier detection where $p(x)$ has low probability}
		\note{What does unsupervised even mean? No prior assumptions, no knowledge at all? Unspecified.. read on this.}
		\note{Notion of truly unsupervised learning is actually harmful to progress.}

	\subsection{Artificial Neural Networks}\label{sec:neuralnetworks}
		Artificial neural networks are a powerful and flexible tool for function approximation. In their principles they are inspired by biological neurophysiology. An artificial network is a model for a function $y = f(x)$ with vector input $x = \{ x_i | i = 1 \ldots n \}$ and vector output $y = \{ y_j | j = 1 \ldots m \}$:
		\begin{equation} \label{eq1}
			\begin{split}
				h_j & =  a (\sum_i w_{ji} x_i + b_i)  \\
				y_j & =  a' (\sum_i w'_{ji} h_i + b'_i)
			\end{split}
		\end{equation},
		with weight matrices $w, w'$, non-linear so-called activation functions $a, a'$ (\eg $a(x)=0$ for $x<0$, $a(x)=x$ for $x>=0$) and bias vectors $b, b'$.
		The components $h_j$ are called hidden units or neurons. Neural networks can also comprise multiple hidden layers via $h_j  =  a (\sum_i w_{ji} h_i + b_i)$.
		It can be shown, that in the limit of infinite hidden units $h_j$ they can approximate any (continuous) function arbitrarily close \cite{cybenko89approx, hornik91approx}.
		In practice, however, networks with more that one layer, referred to as deep neural networks, seem to work better. This may be due to the possibility of building a hierarchical feature representation \cite{zeiler14vis}.

		For processing image data, the weight matrices can be constrained to be only locally connected and to share weights across locations to enforce translation invariance, resulting in \textit{convolutional} neural networks.
		\note{Optimization via gradient descent has proven successful (for deep networks called backpropagation).}
		\note{Need to be differentiable, (differentiable programming).}

\section{Generative Models}\label{sec:genmodel}
	\begin{quote}
	    What I cannot create, I do not understand. - R. Feynman
	\end{quote}
	Learning and understanding structure in data by being able to generate, is the rationale behind generative modelling.
	Generative models are mostly applied for unsupervised learning and can be contrasted to discriminative models. While discriminative models are used to model posterior conditionals $p(y|x)$ (\eg for supervised learning (cf. sec. \ref{sec:supervised}), generative models capture the complete data distribution $p(x)$ in an estimate $\hat p(x)$\todo{cite{bishop06ml}}. Thus, after estimation, one can generate samples from this model $\hat p$. Hence the name generative model.
	\note{Generative modeling can be used for outlier search, where regions with low probability under the model are taken as indicative for an outlier.}
	The currently predominant generative models are built on either autoencoding or adversarial formulations:

	\subsection{Autoencoding Formulations}\label{sec:autoencoding}
		An autoencoding model is learning by reconstructing samples of data, $\hat x = f(x)$. To enforce data compression (otherwise the identity function is a trivial solution of autoencoding) the function has an information bottleneck, namely an inferred latent code $z$ of reduced dimension. The autoencoder is then the chain of an encoding function $z = e(x)$ and a decoding function $\hat x = d(z) = d(e(x))$.

		Whereas the conventional autoencoder consists of deterministic mappings $e, d$, the \textit{variational autoencoder} models the probability distribution $p(x)$. More specifically, it maximizes a lower bound to the logarithmic likelihood $\log p(x)$ of data $x$. This so-called variational lower bound $\mathcal{L}$ is given by:
		% \begin{equation}\label{eq:vae}
			% \mathcal{L} = \underbrace{\mathds{E}_{z\sim q(z|x)}  \log p(x|z)}_{\textrm{reconstruction loss}}  - \underbrace{\textrm{KL}(q(z|x)||p(z))}_{\textrm{regularization}}
		% \end{equation}
		\begin{equation}\label{eq:vae}
			\mathcal{L} = \mathds{E}_{z\sim q(z|x)}  \log p(x|z) - \textrm{KL}(q(z|x)||p(z))
		\end{equation}

		Where $z$ introduces latent variables, with a prior distribution $p(z)$. The approximation to the posterior $q(z|x)$ of the latent variables and the posterior of the data given the latent variables $p(x|z)$. If one wants to model the distributions with neural networks, one typically uses Gaussian distributions and lets the networks predict the parameters (mean $\mu$ and variance $\Sigma$) based on the image.
		In the current machine learning contexts, all functions ($e, d$) and or moments ($\mu, \Sigma$) are modelled with neural networks.

	\subsection{Adversarial Formulations}\label{sec:adversarial}
		\textit{Generative Adversarial Networks} (GAN) \cite{goodfellow14gan} consist of two neural networks competing in a zero-sum game. A generator network $G$ is generating images based on a latent code $z$ sampled from a distribution $p(z)$. The discriminator network $D$ is a binary classifier with the task to classify an image as originating from the data distribution $p_{data}$ or from the distribution produced by $G$. The loss function of $G$ is the negative of the loss of $D$, such that one can formulate the optimization in a minmax form:
		\begin{equation}
			% \begin{split}
				\min_D \max_G - \frac{1}{2} \mathds{E}_{x\sim p_{data}}[\log D(x)] - \frac{1}{2}\mathds{E}_{z\sim p(z)}[\log (1-D(G(z)))]
			% \end{split}
		\end{equation}
		The generator is then optimized to make the output indiscriminable from the data distribution.
		The discriminator can be interpreted as a learned similarity metric, to measure the closeness of an image to the data distribution \cite{larsen15vaegan}.
		There are many variants and extensions to this basic principle of learning with an adversarial task. For example, one can learn a discriminator on for a set of image patches \cite{isola17image2image}.

\section{Disentangling Representations}\label{sec:disentangled}
	In supervised learning, a performance measure is naturally induced by the metric, that is being optimized. In the unsupervised setting, judging the performance of a model is less straightforward.
	\note{For example, when modelling an image domain, one could subjectively rate the quality of the generated image.}
	\todo{introduce latent representation in data compression framing }
	How to rate the quality of the latent representation?

	\subsection{Learning Representations}

	\begin{quote}
		{Disentangle as many factors as possible, discarding as little information about the data as is practical.} - Bengio \etal \cite{bengio13rep} % Y. Bengio, A. Courville and P. Vincent \cite{Bengio2013rep}
	\end{quote}

	According to Bengio \etal \cite{bengio13rep},  a representation is useful, if it can be applied to many - in advance unknown - different tasks, while being trained on only one particular task.
	As the downstream tasks can be multifarious, the essential \textit{information} should be contained in the representation.
	For some tasks only a subset of aspects of the data will be necessary, that is why \textit{disentangled factors} make a representation particularly practical.\\
	The latent representation $z$ learned by generative models captures the essential \textit{information} of the data distribution. That is made sure by requiring the ability to generate samples from the original data distribution from it.
	How then to reach the second goal, the \textit{disentanglement} of generative factors?

	\subsection{Disentangling by Equivariance and Invariance}
		What is a factor? As outlined in the introduction (cf. sec. \ref{sec:intro}), factors in a representation should correspond to elements of the world.
		In general, these factors can interact in complicated ways to finally result in an image. Here, we only consider the case where multiple independent factors each have an influence (cf. Fig. \ref{fig:inferlevel}).
		A change in an element, should then lead to: \emph{i)} a corresponding change in the representational factor and \emph{ii)} leave other factors, that represent other elements, unchanged.
		Formally, this can be seen as inference: a number of latent variables $\mathbf{z_1}\ldots\mathbf{z_N}$ interacted to cause the existence of the observed image $\mathbf{x}$. The task is now to infer estimates for these latent variables $\mathbf{\hat z_i}$. A graphical model of the process is shown in figure \ref{fig:infer}.
		A disentangled representation should simultaneously fulfill equivariance and invariance: A change in $\mathbf{z_i}$ should: \emph{i)} \textit{equivariantly} change in the abstract representational factor $\mathbf{\hat z_i}$, \emph{ii)} while leaving the other factors $\mathbf{\hat z_j}, j\neq i$, that represent other causes, \textit{invariant}.
		\begin{figure}[ht]
			\centering
			\input{fls/tikz/inferlevel.tex}
			\caption{Disentangling causal factors means to infer an estimate - \ie a representation - from an image}
			\label{fig:infer}
		\end{figure}
		\todo{mathematically,.. $f \circ g (x) = ... $}
		\todo{draw other: arbitrary causal}
\end{comment}
\section{Theoretical Impediments from Causality}\label{sec:causality}
	For us, generative factors represent causal elements.
	Learning a disentangled representation is then understood as causal inference from image data.
	In accordance with the causal literature, we can make statements about the type of knowledge, that can be gained by the type of data provided. It turns out that from "raw" image data, raw data meaning images $x$ sampled from $p(x)$, without further assumptions, it is impossible learn a disentangled representation $z$. We start with a short intro to causal learning (\ref{sec:causallearning}), the conclusions for disentangling (\ref{sec:requirements}),

	\subsection{Causal Learning}\label{sec:causallearning}
		\begin{figure}[t]
			\begin{subfigure}{0.3\linewidth}
				\centering
				\input{fls/tikz/causes.tex}
				\caption{}
			\end{subfigure}
			\begin{subfigure}{0.3\linewidth}
				\centering
				\input{fls/tikz/caused.tex}
				\caption{}
			\end{subfigure}
			\begin{subfigure}{0.3\linewidth}
				\centering
				\input{fls/tikz/latentcause.tex}
				\caption{}
			\end{subfigure}
			\caption{Correlation implies causation - if $x_1$ and $x_2$ correlate, a) $x_1$ may cause $x_2$,  b) $x_1$ may be caused by $x_2$ or c) both are contingent on a latent cause $z$}
			\label{fig:reichenbach}
		\end{figure}

		Learning to infer causality is harder than statistical learning. We outline the basic problem for the case of two variables $x, y$: As we outlined above, statistical learning aims at estimating probabilistic properties such as $p(x, y)$ or  $p(y|x)$ from data.
		A well-known theme is that statistical correlation does not imply causation. Less well-known is Reichenbachs common cause principle, that states: if two random variables are statistically dependent, then there exists a third variable that influences both or a direct causal link between them (Fig. \ref{fig:reichenbach}).

		In addition to the probability distribution also the causal structure has to be inferred. \note{Here we will only give a brief summary of results and scope of causal learning.}
		\note{Instead of only learning statistical measures from data, a model also needs to be learned \cite{peters17elements}.}
		% Statistic background $\rightarrow$ correlation is not causation.
		% Reichenbachs principle \cite{reichenbach56time}

		We will start with an example: How to learn the connection between a barometer and the weather from data? If the barometer is working well, there exists a clear correlation between the precipitation and the needle position.
		A highly capable machine learning algorithm that learns only with access to an image dataset showing the barometer and the weather will be able to capture the correlation between needle position and weather condition, but never understand the causal direction, since this is not in the data.
		Imagine how a human would go about solving this problem:
		Having a mechanistic model of the world he could reason about the precise causal mechanism relating weather to humidity to needle position. For example a model of influences (humidity -> barometer)
		What if he has no prior knowledge? A child-level simple solution is to force the needle to move with a finger. The weather will not change. Hence causality has to go other way or via a third latent variable influencing both.
		% - intervening: move barometer needle by hand -> no change in weather, hence causality has to go the other way, (example from \cite{pearl18why})
		% There cannot be an abstract intelligence, which finds out about the world purely by observation. The intelligence has to interact with the world, it has to be in the world.
		% before this becomes too philosophical
		% infer causation from correlation
		% e.g.  RCT

		% lacking the tools to accurately estimate causality, researchers shied away from making causal statement. Developing machines with human-like abilities requires discovery and reasoning in terms of causal models. Recently (in the past 30 years), overshadowed by the prominent success of data-driven deep learning, the field of causality has emerged to mathematical rigor.

		Pearl \cite{pearl18why} distinguishes between three types of questions that can be answered by different types of knowledge:
		\begin{enumerate}
			\item Association. What if I see \ldots?
			\item Intervention. What if I do \ldots?
			\item Counterfactual. What if I had done \ldots?
			%Particularly problematic to learn from data: data consist of facts, not counterfacts.
		\end{enumerate}
		Pearl fundamentally different concepts with different


	\subsection{Disentangling requires Interventions or Model Assumptions}\label{sec:requirements}
		% It can be proven mathematically (Pearl) that interventional data or at least certain (which) causal assumptions about the world are necessary to estimate certain quantities.
		This means that "purely" unsupervised disentangling, \ie estimating $\mathbf{\hat z_i}$ from samples $x \sim p(x)$, is impossible. A rigorous argument for this can be found in \cite{locatello18challenging}.
		% for estimation of causal factors "raw data" insufficient -> need interventional data or model assumptions.
		% so far fitting curve p(x) to data manifold
		Current machine learning operates mostly on the level of association, estimating (complex) correlations from raw data.
		As we have seen, this purely data-driven approach can only go so far.
		In contrast, humans seem to have the ability to interact with their environment and have innate assumptions on coherence, causality, physics etc., which introduce inductive priors.
		To bring \emph{i)} interventions and \emph{ii)} model assumptions to computer vision, we \emph{i)} apply changes to an image, which are assumed to change only one factor and \emph{ii)} model the causal process of the image generation in the theme of analysis-by-synthesis.
		\todo{math}
		% measure: p(x)
		% assume causal model: p(x | a, s)
		% want: p(s | x) and p(a | x)
%
		% encoding
		% $p(s | x )$
		% $p(a | x) = p(a | s, x) p(s | x)$
%
		% decoding
		% $p(x) = p(x | a, s) p(a) p(s)$
%
		% $p(x| do(s), do(a))$

	\subsection{Image Transformation as Intervention}
		\note{we harness intervention}
		% $p(x| do(a), b)$
		\note{in computer vision an intervention is an image transformation if ..}

	\subsection{Analysis-by-Synthesis as Model Assumption}
		\note{what is analysis-by-synthesis?}
		% Inverse graphics
		% Capsules \todo{cite capsules}, Tieleman \cite{tieleman14thesis}
		% make model as good as we can implementing as many assumptions as we can and only leave the rest to powerful model
		% Synthesis known, analysis only indirectly by observing cognition
		%
		% leaving synthesis to learning from scratch, can meet practical/computational limits \eg\ convolutional neural networks better than fully connected neural models.
		% But can also be ultimately impossible. Modelling synthesis explicitly with a causal model about image generation, by knowledge about the physical world enables answering interventional and counter-factual questions. (mathematically impossible to learn from ''pure'' data alone)
