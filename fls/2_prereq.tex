%&tex
\chapter{Prerequisites on Learning Disentanglement}

\section{Learning from Data}
	{Learning from data} is commonly understood as the ability of algorithms to improve their performance on a task with experience accumulated from the observation of data \cite{goodfellow16dlb}. The source of data is usually a dataset - set of data points $X = \{x_i | i \in \{1\ldots n\} \}$, which are sampled from a probability distribution $x_i \sim p(x)$.

	\subsection{Supervised}\label{sec:supervised}
		The term {supervised learning} denotes the task to learn a mapping from data points $x_i$ to target labels $y_i$.
		A supervised algorithm has access to data-label pairs  $(y_i, x_i) \sim p(y, x)$, in order to estimate the connection between data points and labels, either in form of a conditional probability $p(y|x)$, or in form of a deterministic function $y = f(x)$.
		The label $y$ can be either discrete (\eg information about an object class) or continuous (\eg the location of an object part in an image).
		Recent advances, in particular the effectiveness of neural network models (cf. sec. \ref{sec:neuralnetworks}) on big datasets, have led to huge progress on problems that can be formulated as regression or classification. That is why on many traditional computer vision problems, such as \eg object recognition, image classification or human pose estimation, machines are now performing on a superhuman level; hence, these problems are now considered to be essentially solved.\\
		The Achilles' heel of supervised learning lies in the need for a viable supervision signal. To get labels, it is usually required to manually annotate the data. The human effort in this is costly, error-prone and not scalable to the ever-growing vast amounts of raw data.

	\subsection{Unsupervised}\label{sec:unsupervised}
		{Unsupervised learning} is the endeavour to learn about structures and patterns in unlabelled data. In this paradigm, the learning algorithm has access to the samples of the data distribution $x \sim p(x)$. The task is usually framed as a form of density estimation, \ie to model the entire distribution in a probabilistic generative model (cf. sec. \ref{sec:genmodel}).
		Unsupervised learning is considered much harder than supervised learning. There are several complication in the design of unsupervised algorithms:
		\begin{itemize}
			\item Naturally, without supervision, the goal of learning is not specified, hence surrogate objectives have to be formulated. The lack of specification renders the evaluation often arbitrary and subjective\todo{cite evaluation of generative models}.
			\item It is a priori not clear, how much a priori knowledge should be embedded. To introduce no artificial bias, some argue for a purely data-driven approach. Others argue for the importance of certain inductive priors to guide learning \cite{tenenbaum18think}. A related modeling choice is, if the algorithm should be model-free or model-based\todo{cite huszar article}. \note{In this work we argue for using more prior knowledge and modeling assumptions to obtain strong constraints.}
		\end{itemize}
		\todo{connection between unsupervised and supervised learning, \cite{goodfellow16dlb}.}
		\note{A possible framing of the goal is data compression.}
		\note{e.g. outlier detection where $p(x)$ has low probability}
		\note{What does unsupervised even mean? No prior assumptions, no knowledge at all? Unspecified.. read on this.}
		\note{Notion of truly unsupervised learning is actually harmful to progress.}

	\subsection{Artificial Neural Networks}\label{sec:neuralnetworks}
		Artificial neural networks are a powerful and flexible tool for function approximation. In their principles they are inspired by biological neurophysiology. An artificial network is a model for a function $y = f(x)$ with vector input $x = \{ x_i | i = 1 \ldots n \}$ and vector output $y = \{ y_j | j = 1 \ldots m \}$:
		\begin{equation} \label{eq1}
			\begin{split}
				h_j & =  a (\sum_i w_{ji} x_i + b_i)  \\
				y_j & =  a' (\sum_i w'_{ji} h_i + b'_i)
			\end{split}
		\end{equation},
		with weight matrices $w, w'$, non-linear so-called activation functions $a, a'$ (\eg $a(x)=0$ for $x<0$, $a(x)=x$ for $x>=0$) and bias vectors $b, b'$.
		The components $h_j$ are called hidden units or neurons. Neural networks can also comprise multiple hidden layers via $h_j  =  a (\sum_i w_{ji} h_i + b_i)$.
		It can be shown, that in the limit of infinite hidden units $h_j$ they can approximate any (continuous) function arbitrarily close \cite{cybenko89approx, hornik91approx}.
		In practice, however, networks with more that one layer, referred to as deep neural networks, seem to work better. This may be due to the possibility of building a hierarchical feature representation \cite{zeiler14vis}.

		For processing image data, the weight matrices can be constrained to be only locally connected and to share weights across locations to enforce translation invariance, resulting in \textit{convolutional} neural networks.
		\note{Optimization via gradient descent has proven successful (for deep networks called backpropagation).}
		\note{Need to be differentiable, (differentiable programming).}

\section{Generative Models}\label{sec:genmodel}
	\begin{quote}
	    What I cannot create, I do not understand. - R. Feynman
	\end{quote}
	Learning and understanding structure in data by being able to generate, is the rationale behind generative modelling.
	Generative models are mostly applied for unsupervised learning and can be contrasted to discriminative models. While discriminative models are used to model posterior conditionals $p(y|x)$ (\eg for supervised learning (cf. sec. \ref{sec:supervised}), generative models capture the complete data distribution $p(x)$ in an estimate $\hat p(x)$\todo{cite{bishop06ml}}. Thus, after estimation, one can generate samples from this model $\hat p$, hence the name generative model.
	\note{Generative modeling can be used for outlier search, where regions with low probability under the model are taken as indicative for an outlier.}
	The currently predominant generative models are built on either autoencoding or adversarial formulations:

	\subsection{Autoencoding Formulations}\label{sec:autoencoding}
		An autoencoding model is learning by reconstructing samples of data, $\hat x = f(x)$. To enforce data compression (otherwise the identity function is a trivial solution of autoencoding) the function has an information bottleneck, namely an inferred latent code $z$ of reduced dimension. The autoencoder is then the chain of an encoding function $z = e(x)$ and a decoding function $\hat x = d(z) = d(e(x))$.

		Whereas the conventional autoencoder consists of deterministic mappings $e, d$, the \textit{variational autoencoder} models the probability distribution $p(x)$. More specifically, it maximizes a lower bound to the logarithmic likelihood $\log p(x)$ of data $x$. This so-called variational lower bound $\mathcal{L}$ is given by:
		% \begin{equation}\label{eq:vae}
			% \mathcal{L} = \underbrace{\mathds{E}_{z\sim q(z|x)}  \log p(x|z)}_{\textrm{reconstruction loss}}  - \underbrace{\textrm{KL}(q(z|x)||p(z))}_{\textrm{regularization}}
		% \end{equation}
		\begin{equation}\label{eq:vae}
			\mathcal{L} = \mathds{E}_{z\sim q(z|x)}  \log p(x|z) - \textrm{KL}(q(z|x)||p(z))
		\end{equation}

		Where $z$ introduces latent variables, with a prior distribution $p(z)$. The approximation to the posterior $q(z|x)$ of the latent variables and the posterior of the data given the latent variables $p(x|z)$. If one wants to model the distributions with neural networks, one typically uses Gaussian distributions and lets the networks predict the parameters (mean $\mu$ and variance $\Sigma$) based on the image.
		In the current machine learning contexts, all functions ($e, d$) and or moments ($\mu, \Sigma$) are modelled with neural networks.

	\subsection{Adversarial Formulations}\label{sec:adversarial}
		\textit{Generative Adversarial Networks} (GAN) \cite{goodfellow14gan} consist of two neural networks competing in a zero-sum game. A generator network $G$ is generating images based on a latent code $z$ sampled from a distribution $p(z)$. The discriminator network $D$ is a binary classifier with the task to classify an image as originating from the data distribution $p_{data}$ or from the distribution produced by $G$. The loss function of $G$ is the negative of the loss of $D$, such that one can formulate the optimization in a minmax form:
		\begin{equation}
			% \begin{split}
				\min_D \max_G - \frac{1}{2} \mathds{E}_{x\sim p_{data}}[\log D(x)] - \frac{1}{2}\mathds{E}_{z\sim p(z)}[\log (1-D(G(z)))]
			% \end{split}
		\end{equation}
		The discriminator can also be interpreted as a learned similarity metric to measure the closeness of an image to the data distribution \cite{larsen15vaegan}. The generator is then optimized to make the output indiscriminable from the data distribution.

		There are many variants and extensions to this basic principle of learning with an adversarial task. For example, one can learn a discriminator on for a set of image patches \cite{isola17image2image}. \note{add triple gan, ...}

\section{Disentangling Representations}\label{sec:disentangled}
	In supervised learning, a performance measure is naturally induced by the metric that is being optimized. In the unsupervised setting, judging the performance of a model is less straightforward. For example, when modelling an image domain, one could subjectively rate the quality of the generated image. \todo{introduce latent representation in data compression framing }But even for a qualitative assessment the question arises, how to rate the quality of the latent representation?

	\subsection{Learning Representations}

	\begin{quote}
		{Disentangle as many factors as possible, discarding as little information about the data as is practical.} - \cite{bengio13rep} % Y. Bengio, A. Courville and P. Vincent \cite{Bengio2013rep}
	\end{quote}

	According to \cite{bengio13rep} a representation is useful, if it can be applied to many - in advance unknown - different tasks, while being trained on only one particular task.
	As the downstream tasks can be multifarious, the essential \textit{information} should be contained in the representation.
	For some tasks only a subset of aspects of the data will be necessary, that is why \textit{disentangled factors} make a representation particularly practical - so goes their reasoning.\\
	The latent representation $z$ learned by generative models captures the essential \textit{information} of the data distribution. That is made sure by requiring the ability to generate samples from the original data distribution from it.
	How then to reach the second goal, the \textit{disentanglement} of generative factors:

	\subsection{Disentangling as Equivariance and Invariance}
		The
		definition of factor by change static ...
		factors should represent elements of real world

		- change in element -> corresponding change in representational factor
		- leave other factors representing other elements invariant

		Formally, this can be posed as an inference problem: a number of latent variables $\mathbf{z_1}\ldots\mathbf{z_N}$ has interacted in certain ways to cause the existence of the observed image $\mathbf{x}$. An inference algorithm aims at recovering these latent variables from the observation, \ie the image. These recoveries can be seen as estimates $\mathbf{\hat z_i}$ for - or a representation of - the true latent variables $\mathbf{z_i}$. A graphical model of the process is shown in figure \ref{fig:infer}.
		A disentangled representation should then represent each causal element and its state independently: A change in the real causal element $\mathbf{z_i}$ should correspond to an equivalent change in the abstract representational factor $\mathbf{\hat z_i}$, while leaving the other factors $\mathbf{\hat z_j}, j\neq i$, that represent other causes, unchanged.

		\begin{figure}[ht]
			\centering
			\input{fls/tikz/inferlevel.tex}
			\caption{Disentangling causal factors means to infer an estimate - \ie a representation - from an image}
			\label{fig:infer}
		\end{figure}
		mathematically,..
		$f \circ g (x) = ... $


\section{Theoretical Impediments from Causality}\label{sec:causality}
 	\note{factors are causal}
	As outlined earlier, the type of knowledge that can be gained by learning from "raw" data is limited. With raw data we mean data $x$ sampled from a $p(x)$.
	so far fitting curve p(x) to data manifold
	what is missing to human-level intelligence? (cite lake 2016)

	causal learning is a hard problem: instead of only learning statistical measures from data, model also needs to be learned (\cite{peters17elements})


	Hypothesis: disentangling factors = estimating causal factors -> needs causal
	for estimation of causal factors "raw data" insufficient -> need interventional data or model assumptions.
	we do both:
	1. intervene with changes to an image which are assumed to change only one factor.
	2. model the causal process of the image generation in the theme of analysis-by-synthesis

	\subsection{Causal Learning requires Interventions or Assumptions}
	What does the causality literature have to say?
	\begin{figure}[t]
		\begin{subfigure}{0.3\linewidth}
			\centering
			\input{fls/tikz/causes.tex}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.3\linewidth}
			\centering
			\input{fls/tikz/caused.tex}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.3\linewidth}
			\centering
			\input{fls/tikz/latentcause.tex}
			\caption{}
		\end{subfigure}
		\caption{Correlation implies causation - if $x_1$ and $x_2$ correlate, a) $x_1$ may cause $x_2$,  b) $x_1$ may be caused by $x_2$ or c) both are contingent on a latent cause $z$}
		\label{fig:reichenbach}
	\end{figure}
	Statistic background $\rightarrow$ correlation is not causation.
	Reichenbachs principle \cite{reichenbach56time}
	$\rightarrow$ barometer example: How to find out the causal connection between a barometer and the weather. Highly capable machine learning algorithm that learns only with access to an image dataset showing the barometer and the weather. -> will be able to capture the correlation between needle position and weather condition, but never understand causal direction, since it is not in the data.
	Imagine how a human would go about solving this problem. Having a mechanistic model of the world he could reason about the precise causal mechanism relating weather to humidity to needle position.
	- model of influences (humidity -> barometer)
	What if no prior knowledge? A child-level simple solution is to force the needle to move with a finger. The weather will not change. Hence causality has to go other way or third latent variable influencing both.
	- intervening: move barometer needle by hand -> no change in weather, hence causality has to go the other way, (example from \cite{pearl18why})
	There cannot be an abstract intelligence, which finds out about the world purely by observation. The intelligence has to interact with the world, it has to be in the world.
	before this becomes too philosophical
	infer causation from correlation
	RCT

	lacking the tools to accurately estimate causality, researchers shied away from making causal statement. Developing machines with human-like abilities requires discovery and reasoning in terms of causal models. Recently (in the past 30 years), overshadowed by the prominent success of data-driven deep learning, the field of causality has emerged to mathematical rigor.

	- ladder of causation: association, intervention, counter-factual
	- current machine learning mostly on level of association (correlations estimated from "pure" data)
	-> purely data-driven approach can only go so far
	humans seem to have innate assumptions on coherence, causality, physics etc. introducing inductive biases

	\note{multiple levels of interaction, we consider here only one level}
	measure: p(x)
	assume causal model: p(x | a, s)
	want: p(s) and p(a)

	encoding
	$p(s) = p(s | x )$
	$p(a) = p(a | x) = p(a | s, x)$

	decoding
	$p(x) = p(x | a, s) p(a) p(s)$

	p(x| do(s), do(a))

	example: Gaussian
	only with access to p(x)
	hopes to find factors p(a, b) = p(a) p(b) (\cite{chen16infogan}, \cite{higgins16betavae})
	what if not full-filled?
	two-dimensional Gaussian: axis x and y are independent factors.
	in general any superposition of x and y which is orthogonal, can be found
	imagine a perfect dimensionality reduction yielding a  two-dimensional latent space one can find the axes that correlate most with human understanding of independent factors i.e. pose and appearance.
	But how can a machine find these axes automatically from raw data? it cant, neither can anyone (including humans) (Pearl). Humans know these factors are independent from observing that they can change independently e.g. from observing someone undressing or changing his pose (i.e. harnessing temporal information, with the assumption of temporal coherence) or by changing the factors themselves e.g. what happens to the image of me if I change my pullover?
	It can be proven mathematically (Pearl) that interventional data or at least certain (which) causal assumptions about the world are necessary to estimate certain quantities.

	\subsection{Interventions are Transformations}
	we harness intervention
	p(x| do(a), b)
	in computer vision an intervention is an image transformation if ..


	\subsection{Assumptions in Analysis-by-Synthesis}
		Inverse graphics
		Capsules, Tieleman \cite{tieleman14thesis}
	make model as good as we can implementing as many assumptions as we can and only leave the rest to powerful model
	Synthesis known, analysis only indirectly by observing cognition

	leaving synthesis to learning from scratch, can meet practical/computational limits \eg\ convolutional neural networks better than fully connected neural models.
	But can also be ultimately impossible. Modelling synthesis explicitly with a causal model about image generation, by knowledge about the physical world enables answering interventional and counter-factual questions. (mathematically impossible to learn from ''pure'' data alone)



\section{Object Shape and Appearance}
