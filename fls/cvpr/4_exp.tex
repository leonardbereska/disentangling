\section{Experiments}
	In this section we evaluate our unsupervised approach for learning disentangled representation of appearance and shape.
	Sect. \ref{sec:shape} evaluates and visualizes the shape representation on the task of unsupervised landmark discovery.
	Sect. \ref{sec:dis} investigates the disentangling of our representation.
	On the task of conditional image generation, we compare our unsupervised shape/appearance disentanglement performance against a state-of-the-art disentangling method that utilizes groundtruth shape annotations.
	Moreover, on the task of frame-to-frame video translation we show the robustness of our representation across multiple frames.
	Additionally, we evaluate the ability of our method to disentangle parts and their local appearance and shape using a part-wise appearance transfer.

\subsection{Datasets}
    \input{txt/fig_shape0.tex}
	\textbf{CelebA} \cite{Liu:2015vj} contains ca. 200k celebrity faces of 10k identities.
	We resize all images to $128\times 128$ and exclude the training and test set of the MAFL subset, following \cite{Thewlis:2017wi}.
	As  \cite{Thewlis:2017wi, Zhang:2018vz}, we train the regression (to 5 ground truth landmarks) on the MAFL training set (19k images) and test on the MAFL test set (1k images).

	\textbf{Cat Head} \cite{Zhang:2008uj}  has nearly 9k images of cat heads.
	We use the train-test split of \cite{Zhang:2018vz} for training (7,747 images) and testing (1,257 images).
	We regress 5 of the 7 (same as \cite{Zhang:2018vz}) annotated landmarks.
	The images are cropped by bounding boxes constructed around the mean of the ground truth landmark coordinates and resized to $128\times128$.


	\textbf{CUB-200-2011} \cite{Wah:2011vq} comprises ca. 12k images of birds in the wild from 200 bird species.
	We excluded bird species of seabirds, roughly cropped using the provided landmarks as bounding box information and resized to $128\times128$.
	We aligned the parity with the information about the visibility of the eye landmark.
	For comparing with \cite{Zhang:2018vz} we used their published code.


	\textbf{BBC Pose} \cite{Charles:2013tb} contains videos of sign-language signers with varied appearance in front of a changing background. Like \cite{Jakab:2018wc} we loosely crop around the signers.
	The test set includes 1000 frames and the test set signers did not appear in the train set.
	For evaluation, as \cite{Jakab:2018wc}, we utilized the provided evaluation script, which measures the PCK around $d=6$ pixels in the original image resolution.


	\textbf{Human3.6M} \cite{Ionescu:2014ua} features human activity videos.
	We adopt the training and evaluation procedure of \cite{Zhang:2018vz}.
	For proper comparison to \cite{Zhang:2018vz} we also removed the background using the off-the-shelf unsupervised background subtraction method provided in the dataset.


	\textbf{Penn Action} \cite{Zhang:2013tr} contains 2326 video sequences of 15 different sports categories.
	For this experiment we use 6 categories (tennis serve, tennis forehand, baseball pitch, baseball swing, jumping jacks, golf swing).
	We roughly cropped the images around the person, using the provided bounding boxes, then resized to $128\times128$.


	\textbf{Dogs Run} is made from dog videos from YouTube totaling in 1250 images under similar conditions as in Penn Action. The dogs are running in one direction in front of varying backgrounds. The 17 different dog breeds exhibit widely varying appearances.


	\textbf{Deep Fashion} \cite{Liu:2016vv} consists of ca. 53k in-shop clothes images in high-resolution of $256 \times 256$. We selected the images which are showing a full body (all keypoints visible, measured by \cite{Cao:2017vv}) and used the provided train-test split.
	For comparison with Esser \etal \cite{Esser:2018ue} we used their published code.

	\subsection{Evaluating Unsupervised Learning of Shape}\label{sec:shape}
	\input{txt/fig_shape1.tex}
	Fig. \ref{fig:shape} visualizes the learned shape representation.
	To quantitatively evaluate the shape estimation, we measure how well groundtruth landmarks (only during testing) are predicted from it.
	The part means $\mu[\sigma_i(x)]$ (cf. (\ref{eq:equiv})) serve as our landmark estimates and we measure the error when linearly regressing the human-annotated groundtruth landmarks from our estimates.
	For this, we follow the protocol of Thewlis \etal \cite{Thewlis:2017wi}, fixing the network weights after training the model, extracting unsupervised landmarks and training a single linear layer without bias.
	The performance is quantified on a test set by the mean error and the percentage of correct landmarks (PCK).
	We extensively evaluate our model on a diverse set of datasets, each with specific challenges. An overview over the challenges implied by each dataset is given in Tab. \ref{tab:challenges}.
	On all datasets we outperform the state-of-the-art by a significant margin.


	%\textbf{Unsupervised Landmark Prediction for Different Object Classes.}
	\textbf{Diverse Object Classes.}
	On the object classes of human faces, cat faces, and birds (datasets CelebA, Cat Head, and CUB-200-2011) our model predicts landmarks consistently across different instances, cf. Fig. \ref{fig:kp_mania}.
	Tab. \ref{tab:static} compares against the state-of-the-art. Due to different breeds and species the Cat Head, CUB-200-2011 exhibit large variations between instances. Especially on these challenging datasets we outperform competing methods by a large margin.
	Fig. \ref{fig:compare} also provides a direct visual comparison to \cite{Zhang:2018vz} on CUB-200-2011. It becomes evident that our predicted landmarks track the object much more closely. In contrast, \cite{Zhang:2018vz} have learned a slightly deformable, but still rather rigid grid.
	This is due to their separation constraint, which forces landmarks to be mutually distant. We do not need such a problematic bias in our approach, since the localized, part-based representation and reconstruction guides the shape learning and captures the object and its articulations more closely.
	\input{txt/fig_shape2.tex}


	%\textbf{Recovering Articulated Object Pose.}
	\textbf{Articulated Object Pose.}
	Object articulation makes consistent landmark discovery challenging.
	Fig. \ref{fig:kp_mania} shows that our model exhibits strong landmark consistency under articulation and covers the full human body meaningfully.
	Even fine-grained parts such as the arms are tracked across heavy body articulations, which are frequent in the Human3.6M and Penn Action datasets.
    Despite further complications such as viewpoint variations or blurred limbs our model can detect landmarks on Penn Action of similar quality as in the more constrained Human3.6M dataset.
	Additionally, complex background clutter as in BBC Pose and Penn Action, does not hinder finding the object.
	Experiments on the Dogs Run dataset underlines that even completely dissimilar dog breeds can be related via semantic parts.
	Tab. \ref{tab:bbcpose} and Tab. \ref{tab:human} summarize the quantitative evaluations: we outperform other unsupervised and semi-supervised methods by a large margin on both datasets.
	On Human3.6M, our approach achieves a large performance gain even compared to methods that utilize optical flow supervision.
	On BBC Pose, we outperform \cite{Jakab:2018wc} by $6.1\%$, reducing the performance gap to supervised methods significantly.
\subsection{Disentangling Shape and Appearance}\label{sec:dis}
	\input{txt/fig_dis.tex}
	Disentangled representations of object shape and appearance allow to alter both properties individually to synthesize new images. The ability to flexibly control the generator allows, for instance, to change the pose of a person or their clothing. In contrast to previous work \cite{Esser:2018ue, Denton:2017uf, Ma:2017uu, Ma:2017wq, deBem:2018wp, Jakab:2018wc},
	we achieve this ability without requiring supervision \textit{and} using a flexible part-based model instead of a holistic representation. This allows to explicitly control the parts of an object that are to be altered. We quantitatively compare against \emph{supervised} state-of-the-art disentangled synthesis of human figures. Also we qualitatively evaluate our model on unsupervised synthesis of still images, video-to-video translation, and local editing for appearance transfer.


	\textbf{Conditional Image Generation.}
	On Deep Fashion \cite{Liu:2015vj, Liu:2016vv}, a benchmark dataset for supervised disentangling methods, the task is to separate person ID (appearance) from body pose (shape) and then synthesize new images for previously unseen persons from the test set in eight different poses. We randomly sample the target pose and appearance conditioning from the test set. Fig. \ref{fig:allswaps} shows qualitative results.
	We quantitatively compare against supervised state-of-the-art disentangling \cite{Esser:2018ue} by evaluating \emph{i)} invariance of appearance against variation in shape by the re-identification error and \emph{ii)} invariance of shape against variation in appearance by the distance in pose between generated and pose target image.
	\input{txt/fig_dis1.tex}
	\\
	\emph{i)} To evaluate appearance we fine-tune an ImageNet-pretrained \cite{Russakovsky2015imagenet} Inception-Net \cite{Szegedy2015inception} with a re-identification (ReID) algorithm \cite{Xiao:2017} via a triplet loss \cite{Hermans:2017} to the Deep Fashion training set.
	On the generated images we evaluate the standard metrics for ReID, mean average precision (mAP) and rank-1, -5, and -10 accuracy in Tab. \ref{tab:reid}.
	Although our approach is unsupervised it is competitive compared to the supervised VU-Net \cite{Esser:2018ue}.
	\\
	\emph{ii)} To evaluate shape, we extract keypoints using the pose estimator \cite{Cao:2017vv}. Tab. \ref{tab:pose} reports the difference between generated and pose target in percentage of correct keypoints (PCK). As would be expected, VU-Net performs better, since it is trained with exactly the keypoints of \cite{Cao:2017vv}. Still our approach achieves an impressive PCK without supervision underlining the disentanglement of appearance and shape.


	\textbf{Video-to-Video Translation.}
	To evaluate the robustness of our disentangled representation, we synthesize a video sequence frame-by-frame without temporal consistency constraints. On BBC Pose \cite{Charles:2013tb}, one video provides a sequence of target poses, another video a sequence of source appearances to then perform retargeting, Fig. \ref{fig:bbcthumb}.
	Although there is no temporal coupling, the generated sequences are smooth and pose estimation is robust. Secondly, the training on the natural spatial deformations in video data enables the model to encapsulate realistic transitions such as out-of-plane rotation and complex 3D articulation of hands and even fingers.


	\textbf{Part Appearance Transfer.}
	The flexible part-based representation allows to explicitly control local appearance. Fig. \ref{fig:partswaps} shows swaps of appearance for shirt, pants, etc. In contrast to holistic representations \cite{Esser:2018ue, Jakab:2018wc, Ma:2017uu, Ma:2017wq, deBem:2018wp}, we can guarantee the transfer to be focused on selected object parts.
	\input{txt/fig_dis2.tex}
