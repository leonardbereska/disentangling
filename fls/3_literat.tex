%&tex
% \chapter{Literature Review: Disentangling}
\chapter{Analysis of Literature on Disentangling}\label{sec:literature}
	% \note{Goal of this section: analyse disentangling (with causal insights, situatate method within this
	In the following, we analyze the computer vision literature on disentangling generative factors \wrt the causal inference insights (cf. sec. \ref{sec:causality}).
	In this thesis the focus is on disentangling the factors of object shape and appearance and therefore we also review the unsupervised shape learning.
	\note{focusing on factors makes sense, because arbitrary factors impossible (cite schoelkopf)}
	\todo{research for papers connecting disentangling and causality}


\section{Analysis-by-Synthesis}
	Analysis-by-synthesis is a theme that originates from the research on language perception \eg \cite{bever10anabysyn}, but has also been successfully applied to visual perception.
	The idea is, to guide the perception (analysis) by a model for generation (synthesis).
	In computational vision this can be formulated in autoencoder models \cite{tieleman14thesis}.
	In vision, the domain-specific knowledge about the image generation (decoding) exceeds the knowledge about computational cognition (encoding).
	Hence, the domain-specific knowledge is used to constrain the decoder, while the encoder is generic.
	\note{This is the reason why analysis-by-synthesis makes sense in the first place.}
	\note{A related idea are capsules, which encode the geometric parameters (as generative factors) of entities like objects or object parts \cite{sabour17capsule}. transforming ae \cite{hinton11tranformae}}
	For disentangling generative factors the analysis-by-synthesis scheme has been applied to computer vision earlier \cite{kulkarni15dcign, yildirim15anabysyn, desjardins12genentangle}, however with the use of label information for the factors.
	In contrast, we apply image transformations to emulate the labels.
	Additionally, we choose a specific model for the interaction of shape and appearance with a local part-based model on shape and a corresponding part appearance that is linked to a part location.



\section{Disentangled Generative Models}
	In order to gain a conceptual understanding of the world, disentangling the underlying factors of variation is a crucial step, as has been argued in numerous works,~\cite{desjardins12genentangle, bengio13rep, chen16infogan, higgins16betavae, eastwood18disquantify}.
	Capturing essential information about data in a representation by being able to generate it is the rationale behind generative modelling.
	Currently the approaches in this direction are defined by adversarial \cite{goodfellow16dlb} and autoencoding \cite{kingma13vae} model formulations.
	Recently, the endeavour for disentangling explanatory factors in the latent representation of generative models is being made explicit in the objective functions \cite{burgess18betavae, chen16infogan} of these models. So far, however, these attempts are limited to rigid objects without articulation and disentangle holistic image factors like illumination, object rotation or total shape and global appearance \cite{denton17disvideo}.
	\note{Generic approaches are \cite{worrall17encdec, li18analogy}}


\section{Part-based Representation Learning}
	\note{parts as regional attention (cite attention paper)}
	\note{parts/compositionality is key to creativity -> new combination of known parts}

	Describing an object as an assembly of parts is a classical paradigm for learning an object representation in computer vision \cite{ross06parts} with linkage to human perceptual theories \cite{biederman87recognition}.
	What constitutes a part, is the defining question in this scheme.
	Defining parts by e.g. (\emph{i}) visual/semantic features (object detection), or by (\emph{ii}) geometric shape, behavior under (\emph{iii}) viewpoint changes or (\emph{iv}) object articulation, in general leads to a different partition of the object.
	Recently, most part learning has been employed for object recognition, such as in \cite{felzenszwalb10dpm, novotny17anchornet, singh12patch, mesnil13partssemantic, yang16dpmpose, lam17finerecognize}.
	% other methods for discriminative -> recognition -> focus on visual factorization % ours for generative image modeling -> also structural aspects %\cite{DPM, constellation1,constellation2} combine parts with a probabilistic prior on their spatial arrangements. %Novotny et al. \cite{anchornet} find parts shared by similar categories to perform semantic matching. Mining of discriminative parts can be formulated as an clustering problem or an iterative refinement process \cite{partLearning2}.
	To solve such a discriminative task, parts will be based on the semantic connection to the object and can ignore their spatial arrangement and articulation of the object instance. Our method instead is driven by a generative process and aims at more generic modeling of the object as a whole. Hence, parts have to encode both spatial structure and visual appearance accurately. To our best knowledge unsupervised part learning and the proposed split in shape and appearance description for a part has only been used in pre-deep learning approaches \cite{ross06parts, nguyen13nonnegative, cootes98activeappear}.


% \section{Landmark Learning}
\section{Unsupervised Learning of Object Shape}
	\note{for estimating shape $s$ from images $x$ the task is
	$p(s|x)$}
	\note{representation of shape can be landmarks}

	There is an extensive literature on landmarks as compact representations of object shape.
	Most approaches, however, make use of manual landmark annotations as supervision signal. Landmark learning has been applied to human faces \cite{wu17faceocclu, ranjan16hyperface, yu16deform, zhang16facealign, zhu15facecoarse, zhang14facemultitask, pedersoli14facedeform} and bodies \cite{ionescu11posestructured, toshev14deeppose, pfister15flowingconv, wei16posemachine, newell16hourglass, lim18posetransform, cao17affinityfield}.

	To tackle the problem without supervision, Thewlis \etal \cite{thewlis17} proposed enforcing equivariance of landmark locations under artificial transformations of images. The equivariance idea had been formulated in earlier work \cite{lenc16covariant} and has since been extended to learn a dense object-centric coordinate frame \cite{thewlis17dense}. However, enforcing only equivariance encourages consistent landmarks at %easily
	discriminable object locations,
	but disregards an explanatory coverage of the object.
	% which is especially critical for strong
	%does not aim at explaining the object from it.

	Zhang \etal \cite{zhang18} addresses this issue: the equivariance task is supplemented by a reconstruction task in an autoencoder framework, which gives visual meaning to the landmarks. However, in contrast to our work, he does not disentangle shape and appearance of the object. Furthermore, his approach relies on a separation constraint in order to avoid the collapse of landmarks.
	This constraint results in an artificial, rather grid-like layout of landmarks, that does not scale to complex articulations.
	%In contrast, our method disentangles shape and appearance. Hence, for optimal reconstruction, our model has to make use of the shape information efficiently, which leads to a meaningful coverage.

	%Building on the reconstruction objective introduced by \cite{Zhang:2018vz},
	Jakab \etal \cite{jakab18} proposes conditioning the generation on a landmark representation from another image. A global feature representation of one image is combined with the landmark positions of another image to reconstruct the latter. Instead of considering landmarks which only form a representation for spatial object structure, we factorize an object into local parts, each with its own shape \textit{and} appearance description.
	Thus, parts are learned which meaningfully capture the variance of an object class in shape as well as in appearance.

	Additionally, and in contrast to all these works (\cite{thewlis17, zhang18, jakab18}) we take the extend of parts into account, when formulating our equivariance constraint. Furthermore, we explicitly address the goal of disentangling shape and appearance on a part-based level by introducing invariance constraints.
	%
	%
	%Jakab \etal \cite{Jakab:2018wc} proposes conditioning the generation on a landmark representation from another image. A global feature representation of one image is combined with the landmark positions of another image to reconstruct the latter.
	%In contrast to our work, appearance representation is global, whereas we partition an object explicitly into local parts, each with its own shape \textit{and} appearance description. The conceptual difference here is, that we understand a part not only as a point (landmark), but as an image region, with an appearance description for this region. This %further factorization
	%encourages part placement at visually meaningful locations and assists the part assignment consistency.
	%\\
    %In contrast to all these works (\cite{Thewlis:2017wi, Zhang:2018vz, Jakab:2018wc}) we model shape by parts with an extend. That extend needs to be equivariant up to the second moment and is used to extract the appearance of a part, which leads to better object coverage. In addition, we make the goal of disentangling explicit with our formulation and show quantitative results on this.
    %\textbf{Landmark learning.}
	%There is an extensive literature on landmarks as compact representations of object structure.
	%Most approaches, however, make use of manual landmark annotations as supervision signal \cite{Wu:2017vc, Ranjan:2016vv, Yu:2016vi, Zhang:2016vx, Zhu:2015tz, Zhang:2014wy, Pedersoli:2014ta, Ionescu:2011ue, Toshev:2014tp, Pfister:2015uo, Wei:2016ws, Newell:2016vq, Lim:2018uo, Cao:2017vv}.
	%\\
	%To tackle the problem without supervision, Thewlis \etal \cite{Thewlis:2017wi} proposed enforcing equivariance of landmark locations under artificial transformations of images. The equivariance idea had been formulated in earlier work \cite{Lenc:2016tz} and has since been extended to learn a dense object-centric coordinate frame \cite{Thewlis:2017wg}. However, enforcing only equivariance encourages consistent landmarks at easily discriminable object locations, but disregards a sensible coverage of the object.
	%\\
	%Zhang \etal \cite{Zhang:2018vz} addresses this issue: the equivariance task is supplemented by a reconstruction task in an auto-encoder framework, which gives visual meaning to the landmarks. However, their approach does not disentangle shape from appearance, thus shape information can be encoded in the appearance representation resulting in the collapse of the landmarks. To counter this they rely on a artificial separation constraint which results in an artificial, rather grid-like layout of landmarks, that does not scale to complex variation in object shape.
	%In contrast, our method disentangles shape and appearance of the object and thus, for optimal reconstruction, our model has to capture the variation of object shape in the shape representation which automatically leads to a meaningful object coverage.
	%\\
    %Jakab \etal \cite{Jakab:2018wc} proposes conditioning the generation on a landmark representation from another image. A global feature representation of one image is combined with the landmark positions of another image to reconstruct the latter. Instead of considering landmarks which only form a representation for spatial object structure, we factorize an object into local parts, each with its own shape \textit{and} appearance description.
	% Thus, parts are learned which meaningfully capture the variance in shape as well as in appearance of an object class.
    %In contrast to all these works (\cite{Thewlis:2017wi, Zhang:2018vz, Jakab:2018wc}) we take the extend of parts into account when formulation the equivariance constraint.
    % In addition, we make the goal of disentangling shape and appearance on a part based level explicit with our formulation and show quantitative results on this.

\section{Disentangling Shape and Appearance}
	Factorizing an object representation into shape and appearance is a popular ansatz for representation learning.
	Recently, a lot of progress has been made in this direction by conditioning generative models on shape information \cite{esser18, ma17poseguided, debem18dgpose, ma17disperson, siarohin18deformgan, balakrishnan18unseenposes}.
	While most of them explain the object holistically, only few also introduce a factorization into parts \cite{siarohin18deformgan, balakrishnan18unseenposes}.
	In contrast to these shape-supervised approaches, we learn both shape and appearance without any supervision.

	For unsupervised disentangling, several generative frameworks have been proposed~\cite{higgins16betavae, chen16infogan, li18analogy, denton17disvideo, shu18shapeappear, xing18shapeappear}.
	However, these works use holistic models and show results on rather rigid objects and simple datasets, while we explicitly tackle strong articulation with a part-based formulation.
	\note{shu different understanding of shape: shape as surface not as arrangement of parts}

